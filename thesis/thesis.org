#+TITLE:
#+DATE:
#+AUTHOR:
#+EMAIL:
#+OPTIONS: `:nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:nil c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:t toc:nil
#+OPTIONS: todo:t |:t
#+CREATOR:
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export
#+OPTIONS: texht:t
#+LATEX_CLASS: clmthesis
#+LATEX_CLASS_OPTIONS:

#+LATEX_HEADER_EXTRA: \lstset{
#+LATEX_HEADER_EXTRA:   numbers=left,
#+LATEX_HEADER_EXTRA:   stepnumber=1,
#+LATEX_HEADER_EXTRA:   firstnumber=1,
#+LATEX_HEADER_EXTRA:   numberfirstline=true,
#+LATEX_HEADER_EXTRA:   captionpos=b,
#+LATEX_HEADER_EXTRA:   basicstyle=\footnotesize,
#+LATEX_HEADER_EXTRA:   extendedchars=true,
#+LATEX_HEADER_EXTRA:   literate= {->}{{$\rightarrow$}}1 {-->}{{$\rightarrow$}}2
#+LATEX_HEADER_EXTRA:             {µ}{{$\mu$}}1 {⋅}{{$\cdot$}}1
#+LATEX_HEADER_EXTRA:             {o--}{{\rule[0.5ex]{3mm}{0.1mm}}}2,
#+LATEX_HEADER_EXTRA: }
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: \lstdefinestyle{base}{
#+LATEX_HEADER_EXTRA:   emptylines=1,
#+LATEX_HEADER_EXTRA:   moredelim=**[is][keywordstyle]{**}{**},
#+LATEX_HEADER_EXTRA:   morecomment=[l]{{-- }},
#+LATEX_HEADER_EXTRA:   % commentstyle=\itshape\color{green!30!black},
#+LATEX_HEADER_EXTRA: }
#+LATEX_HEADER_EXTRA:
#+LATEX_HEADER_EXTRA: \lstdefinestyle{base-nocomments}{
#+LATEX_HEADER_EXTRA:   emptylines=1,
#+LATEX_HEADER_EXTRA:   moredelim=**[is][keywordstyle]{**}{**},
#+LATEX_HEADER_EXTRA: }

#+LATEX_HEADER_EXTRA: \newcommand{\compresslist}{
#+LATEX_HEADER_EXTRA:   \setlength{\itemsep}{1pt}
#+LATEX_HEADER_EXTRA:   \setlength{\parskip}{0pt}
#+LATEX_HEADER_EXTRA:   \setlength{\parsep}{0pt}
#+LATEX_HEADER_EXTRA: }


# +LATEX_HEADER: \include{tikz, wrapfig}
# +LATEX_HEADER: \usepackage{graphicx}
#  \numberwithin{figure}{section}
# +latex_header: \hypersetup{hidelinks}
# +LATEX_HEADER: \usepackage[cm]{fullpage}
# +LATEX_HEADER: \usepackage{caption}
# +LATEX_HEADER: \usepackage{subfigure}
# +LATEX_HEADER:  \captionsetup{singlelinecheck=off,labelfont=bf}
# +LATEX_HEADER: \linespread{1.5}

# +LaTeX_HEADER: \usepackage[numbers]{natbib}
# +latex_header: \usepackage{repeatably}
# +latex_header: \usepackage{color, colortbl}
# +latex_header: \usepackage{multirow, comment}
#+latex_header: \usepackage[numbers]{natbib, stmaryrd, tikz}
#+latex_header: \usepackage{amortised,listings,enumitem,xcolor}
#+latex_header: \usepackage{mathtools,proof,amsthm,amsmath,amssymb,bussproofs,turnstile}


#+CONSTANTS:

\bibliographystyle{plainnat}

\abstract{ Amortized resource analysis yields highly accurate resource complexity bounds which are commonly much better than the ones by other resource analysis techniques, like the worst-case analysis. During the last decades techniques for amortized resource analysis were developed and transformed to be compatible with arbitrary data structures and (typed) Term Rewrite Systems. This seminar report briefly introduces these developments and shows transformation of the analysis methods to TRSs. Further, the current status of the implementation for the univariate polynomial analysis is presented. }


\author{Manuel Schneckenreither}
\mailaddress{manuel.schneckenreither@student.uibk.ac.at}
\matriculationnumber{1117198}
\supervisor{Assoc.~Prof.~Dr.~Georg Moser}
\date{\today}
\title{Amortized Resource Analysis for Term Rewrite Systems}

\maketitle
\tableofcontents


* Introduction

Quantitative analysis of algorithms, especially according to their execution time,
are important when comparing algorithms, designing efficient programs and to
identify performance bottlenecks in software \cite{hoffmann2012resource}. In recent
years amortized resource analysis techniques have been developed and advanced in
several publications.

In 2000 \citeauthor{hofmann2000type} presented in \cite{hofmann2000type} the basic
ideas for the amortized resource analysis: A resource type was introduced that
controls the number of constructor symbols in recursive functions and ensures
linear heap space. However, this heap-space analysis is restricted to data
structures composed of lists, binary trees and pairs. The result of the paper
\cite{hofmann2000type} is a compilation process of a linear typed first-order
functional programming language (LFPL) into malloc()-free C code. They show that
the program will run in linear heap-size according to the input paramters.
Similarly in \cite{hofmann2003static} and \cite{hofmann2006type}
\citeauthor{hofmann2006type} convert the concepts for the heap-size analysis of
\cite{hofmann2000type} to a first-order typed functional language and respectively
Featherweight Java programs \cite{igarashi1999featherweight} using resource
annotations. In 2006 in \cite{hofmann2006type} they first used the notation of
potential, which was utilized in the later works and thus forge the bridge to
amortized resource analysis as it was introduced by \citeauthor{tarjan85} in 1985
\cite{tarjan85}.

In 2009 \citeauthor{Jost09carboncredits} improved the previous version for
functional programs such that arbitrary (recursive) data structures can be used and
also lifted the method to incorporate worst-case runtime bounds
\cite{Jost09carboncredits}. The experiment results show astonishing accurate
predictions when compared to actual resource usage but the analysis method are
still limited to linear bounded input programs. Later,
\citeauthor{hoffmann2012multivariate} reduce the limitations by presenting an
analysis which is capable of handling multivariate bounds
\cite{hoffmann2012multivariate}. Nonetheless, the input programs are still
restricted to a first-order fragment of OCaml featuring integers, lists, binary
trees, and recursion. This OCaml fragment is named Resource Aware ML (RaML).

In 2014 \citeauthor{hofmann2014amortised} converted the techniques of the
univariate analysis to typed Term Rewrite Systems (typed TRS) resulting in
polynomial bounds on the innermost runtime complexity of the given typed TRS
\cite{hofmann2014amortised}, before lifting the analysis to gain multivariate
bounds for typed TRS in \cite{hofmann_et_al:LIPIcs:2015:5167}. The most important
improvement to \cite{Jost09carboncredits} and \cite{hoffmann2012multivariate} is
that arbitrary data types can be analyzed with the newly demonstrated methods.

This seminar report focuses on the transformation of the techniques developed for
RaML, especially those presented in \cite{Jost09carboncredits}, such that they can
be applied to TRS as explained in \cite{hofmann2014amortised}. Furthermore, we
describe the current implementation of the univariate analysis for typed TRS from
\cite{hofmann2014amortised} and point out differences to the implementation of the
RaML prototypes as described in \cite{hoffmann2011types}.

For the rest of the seminar paper we assume familiarity with the notations and
analysis methods presented in the publications \cite{Jost09carboncredits} and
\cite{hofmann2014amortised}, as well as basic knowledge of Term Rewrite Systems
(TRS).

The rest of the seminar report is structured as follows. Section [[Theory]] explains
the most important transformations and differences of the methods presented in
\cite{Jost09carboncredits} and \cite{hofmann2014amortised}. Section [[Example]] shows,
by an example, the different input formats to the tools and explains the output of
the implementations. Then, Section [[Implementation]] summarizes the main concepts of
the tool that implements the theory of \cite{hofmann2014amortised}. Section
[[Preliminary Experiments]] introduces some preliminary results to get a feeling for
the execution times of the analyses. Next, Section [[Future Work]] displays an outlook
for the future, before we conclude in Section [[Conclusion]].

* Theory

This section explains the transformation and improvements of the univariate linear
bound analysis presented in \cite{Jost09carboncredits} to gain the univariate
polynomial bound analysis method for typed TRS introduced in
\cite{hofmann2014amortised}. Thus, this section heavily depends on content,
and adapts concepts and ideas from \cite{Jost09carboncredits} and
\cite{hofmann2014amortised}.


** Typing Rules

The heart of the analysis are the typing rules. This subsection is concerned with
the evolution of the typing rules.

As a recap, the typing judgment ${\tjudge{\Gamma}{p}{q}{\typed{t}{A}}}$ specifies
that for all valuations $\nu$ the expression $t$ has type $\nu(A)$ under the typing
context $\Gamma$ (which maps identifiers to types). Moreover, evaluating $t$ under
the context $\Gamma$ requires at most a potential $p +
\Phi(\typed{\sigma}{\Gamma})$ and leaves at least a potential $q +
\Phi(\typed{\sigma}{\Gamma})$. Whereas, $\sigma$ is a substitution, such that for
all $x \in \dom(\Gamma)\ x\sigma$ is of type \Gamma(x). Thus, the costs for
evaluating the expression $\typed{t}{A}$ under context $\Gamma$ is bounded from
above by $p-q$.

As an example Figure \ref{fig:09_tr_fun} shows the typing rule for a function call
as it was presented in \cite{Jost09carboncredits}. The typing context $\Gamma$ is
the set $\{ \typed{y_1}{A_1}, \ldots, \typed{y_k}{A_k} \}$, \textsf{fid} is the
function call identifier, further the function is called with arguments $y_1,
\ldots, y_k$. The type of the expression is $C$ which already includes the
annotations that specify the potential of this type. The constraints over resource
variables specified in $\psi$ must hold when the rule is applied. Further, the
function signature $\Sigma(\mathsf{fid})$ must equal the quadruple containing the
expression $e_{fid}$ that needs to be proven well-typed, the variables $y_1,
\ldots, y_k$, the signature of this function call, and the given constraints
$\psi$. The application of the rule is restricted to functions with at least one
argument. The costs for the function call are $p - p'$, then $Kcall(k)$ and
$KCall'(k)$ represent the absolute costs of setting up before the call and clearing
up after the call.

#+CAPTION:     Typing rule for a function call from \cite{Jost09carboncredits}.
#+LABEL:      fig:09_tr_fun
#+ATTR_LaTeX: :width 0.7\textwidth
[[file:./figures/09_typing_rule_fun.png]]


The typing rules of \cite{Jost09carboncredits} can be transformed to be compatible
with typed TRSs. Figure \ref{fig:note_tr_fun} shows the function application rule
for typed TRSs. In this representation the constraints over the resource variables
are left out, further every defined function symbol has only one allowed function
signature. Thus, the signature map does not store quadruples anymore, but annotated
signatures. Further, the fixed costs $Kcall(k)$ and $Kcall'(k)$ have been dropped
as for typed TRSs only evaluation steps are analyzed, but no memory structures.

#+CAPTION:    Tying rule for a function call transformed to typed TRSs.
#+LABEL:      fig:note_tr_fun
#+ATTR_LaTeX: :width 0.5\textwidth
[[file:./figures/note_tr_fun.png]]

This direct transformation was done for all of the typing rules. Gaining the typing
system for a runtime analysis of linear bounded typed TRSs as given in Figure
\ref{fig:note_ts}. This system differentiates between function calls of defined
function symbols $f$ and constructor symbols $c$. Furthermore, the \textsf{CASE}
and \textsf{LET} rules were replaced by a function composition rule, which handles
function calls to arbitrary terms.

#+CAPTION:    Type system for linear bounded typed TRSs.
#+LABEL:      fig:note_ts
#+ATTR_LaTeX: :width 1.0\textwidth
[[file:./figures/note_ts.png]]

\pagebreak

In further developments the attention was put onto the analysis of runtime of typed
TRSs. Therefore, the potential below the judge, that is the potential that is given
back after an evaluation of the respective expression, was removed, as runtime
cannot be passed back when once utilized. These improved typing rules are shown in
Figure \ref{fig:14_ts} and were presented in \cite{hofmann2014amortised}. The
obvious change is that the constructor rule was removed and merged with the
function call rule. This reduces complexity and removes the need of explicit data
constructors for TRSs. Thus, these rule might further be simplified by just
replacing the types (which include the annotations) with the annotations
themselves. This does not change the runtime complexity of the given TRS
\cite{avanzini}. However, heuristics that may utilize these types, e.g. by
detecting the recursive parts of functions cannot be used when the types are fully
erased from the analysis. An example on how to perform this transformation is given
in Figure \ref{fig:nn_tr_fun}, showing the function application rule without data
types. Another important improvement of the types system is that the scalar
annotations were replaced by vectors. The length of the vectors that are needed to
type the system then represent the degree of the polynomial that is needed to bound
the runtime of the input typed TRS, cf. Theorem 3.2 of \cite{hofmann2014amortised}.

#+CAPTION:    Type system for univariate polynomial bounded TRSs \cite{hofmann2014amortised}.
#+LABEL:      fig:14_ts
#+ATTR_LaTeX: :width 1\textwidth
[[file:./figures/14_ts.png]]


#+BEGIN_LaTeX
\begin{figure}
\begin{center}
  \begin{tabular}{c}
  $\infer{\tjudge{\typed{x_1}{\vec{u_1}},\dots,\typed{x_n}{\vec{u_n}}}{p}{}{\typed{f(x_1,\dots,x_n)}{\vec{v}}}}{%
      f \in \CS \cup \DS
      &
      \vec{u_1} \times \cdots \times \vec{u_n} \xrightarrow{p} \vec{v} \in \FS(f)
    }$
  \end{tabular}
\end{center}
\caption{\label{fig:nn_tr_fun}Further development of function call type rule by removing the need of types.}
\end{figure}
#+END_LaTeX


\pagebreak[4]

** Well-Typedness

Clearly as the type system evolved also the specification for well-typedness
changed. Figure \ref{fig:09_wt} shows the well-typedness constraint as it was given
in \cite{Jost09carboncredits}. It means, if a signature as defined by $\Sigma$ is
used for the function \textsf{fid}, then there must be a derivation using the type
system from above proving the well-typedness of this signature. Note the reversed
signs for the calls to $Kcall(a)$ and $Kcall'(a)$ as opposed to the function
application typing rule from before. Here, $p$ and $p'$ must be large enough to be
able to evaluate the function call, which needs $Kcall(a)$ resources when executed.
All functions defined in the input program have to be derived by the type system
and must coincide with the well-typedness constraints.

#+CAPTION:    Well-typedness as defined in \cite{Jost09carboncredits}.
#+LABEL:      fig:09_wt
#+ATTR_LaTeX: :width 0.9\textwidth
[[file:./figures/09_wt.png]]

These basic ideas were again transformed to fit for TRSs. Figure \ref{fig:14_wt}
shows the well-typedness constraints as presented in \cite{hofmann2014amortised}.
All defined function symbols of the input TRS must admit these constraints to
ensure the system is well-typed, whereas for all rules $f(l_1,\ldots,l_n) \to r$
the variables of the left-hand side (lhs) are $\Var(f(l_1,\ldots,l_n)) = \{ y_1,
\ldots, y_l \}$. These constraints must be derivable for all signatures
$\funtype{[A_1 \times \cdots \times A_n]}{C}{p}{} \in \FS(f)$, for all annotated
types $B_j$ ($j \in \{1, \ldots, l\}$), and costs $k_i$, such that
$\tjudge{\typed{y_1}{B_1}, \ldots, \typed{y_l}{B_l}}{k_i}{}{\typed{l_i}{A_i}}$ is
derivable without using the relax rule, which could otherwise decrease the costs
$k_i$. Here, the potentials of the constructors of the lhs are used to sum up the
actual potential of the lhs of the rewrite rule. Further, the $-1$ corresponds to
the $Kcall(a)$ in \cite{Jost09carboncredits} and specifies the application of the
given rewrite rule, gaining the right-hand side (rhs) $r$. Informally, the $-1$
specifies that one step in the derivation is happening, whenever the rewrite rule
is applied. Thus, the potential must at least decrease by $1$, leaving the rhs with
less potential than the lhs.


#+BEGIN_latex
\begin{figure}[h!]
\begin{equation*}
\tjudge{\typed{y_1}{B_1}, \ldots, \typed{y_l}{B_l}}{p-1+\sum_{i=1}^nk_i}{}{\typed{r}{C}}
\end{equation*}
\caption{\label{fig:14_wt} These are the well-typedness constraints as presented in \cite{hofmann2014amortised}.}
\end{figure}
#+end_latex


** Potential Function

As keeping track of the potential (or credits as named in
\cite{Jost09carboncredits}) is the main concept of this amortized analysis the
function was kept as it was. Figure \ref{fig:14_pot} shows the main design of the
function: The potential is recursively summed up for the given type with the
corresponding annotated data type. It is easy to see that again the data types
cannot bring an advantage for the analysis over simple vectors.


#+BEGIN_LaTeX
\begin{figure}[h!]
\begin{align*}
\Phi(\typed{t}{C}) = p + \Phi(\typed{t_1}{A_1}) + \cdots + \Phi(\typed{t_n}{A_n}).
\end{align*}
\caption{\label{fig:14_pot}The potential function recursively sums up the potential of an expression with the associated type. Here the term $t$ is supposed to consist of a function (maybe without arguments) of the shape $f(t_1, \ldots, t_n) \in \TA(\CS \cup \DS)$ and admits the signature $\funtype{[A_1 \times \cdots \times A_n]}{C}{p}{}$.}
\end{figure}
#+END_LaTeX

To be able to correlate the length of the vectors used with the runtime of the
given TRS \citeauthor{hofmann2014amortised} introduced Theorem 3.2
\cite{hofmann2014amortised}. This Theorem bounds the growth of the potential of the
constructors, such that the runtime complexity $\mathsf{rc}_{\mathcal{R}}(n) \in
O(n^k)$, where $n = |v|$ and $k$ is the vector length used. Thus, if this theorem
is satisfied, the length of the vector specifies the degree of the polynomial
needed to bound the runtime complexity of the input system from above
\cite{hofmann2014amortised}.

** Superposition and Uniqueness

Problems arouse by merging the constructor rule with the function rule. This
merging would allow different annotations for the same constructor terms. That
means, the same terms could have different potential values. This is obviously not
desired at it would allow ambiguity of the potential. Thus, the so called
superposition and uniqueness properties were introduced in
\cite{hofmann2014amortised}.

#+BEGIN_DEFINITION
The /uniqueness property/ is defined over the set of annotated signatures for a
constructor $f \in \CS$. If $f$ has the result type $C$, then for each annotation
$C^\vect{q}$ there should exist exactly one declaration of the form
$\funtype{[A_1^{\vect{p_1}} \times \cdots \times
A_n^{\vect{p_n}}]}{C^{\vect{q}}}{p}{}$ in $\FS(f)$.[fn:: Note that in
\cite{hofmann2014amortised} the uniqueness property is defined for functions $f \in
\CS \cup \DS$. However, as uniqueness for functions $f \in \DS$ is only needed for
the polynomial interpretations, this definition is simplified by removing the set
of defined function symbols $\DS$ from the definition.]
#+END_DEFINITION

#+BEGIN_DEFINITION
The /superposition principle/ states that if a constructor $c$ admits the
annotations $\funtype{[A_1^{\vect{p_1}} \times \cdots \times
A_n^{\vect{p_n}}]}{C^{\vect{q}}}{p}{}$ and $\funtype{[A_1^{\vect{p'_1}} \times
\cdots \times A_n^{\vect{p'_n}}]}{C^{\vect{q'}}}{p'}{}$ then it also has the
annotations $\funtype{[A_1^{\lambda\vect{p_1}} \times \cdots \times
A_n^{\lambda\vect{p_n}}]}{C^{\lambda\vect{q}}}{\lambda p}{}$ with $\lambda
\geqslant 0$ and $\funtype{[A_1^{\vect{p_1} + \vect{p'_1}} \times \cdots \times
A_n^{\vect{p_n}+\vect{p'_n}}]}{C^{\vect{q}+\vect{q'}}}{p+p'}{}$.
#+END_DEFINITION

# Put differently, for any annotated constructor signature with at least one
# annotation vector being non-zero, the constructor has to admit all linear
# combinations of this signature also.


* Example

This section shows the differences of the tools and input programs by an example.
Listing \ref{lst:raml_rev} represents the \textsf{reverse} function in RaML code.
The \textsf{append} function appends two lists together. This is done by
recursively calling \textsf{append} on the tail of the first list. This
implementation is used in real world code, for instance in Haskell, \textsf{append}
(\textsf{++}) is implemented exactly like this[fn:: See
\url{http://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.Base.html}.]. The
second function is \textsf{reverse}, which takes as input parameter a list and
returns another list with the elements in opposite order. To reverse the given list
it first calls itself on the tail of the input list, before calling \textsf{append}
for each element of the input list once. As \textsf{append} is recursive on the
same parameter as \textsf{reverse} is, the runtime complexity of this program can
be bound at best by a 2^{\text{nd}}-degree polynomial. The web-interface of
RaML[fn:: Available at \url{http://www.raml.co/}.], which is by now also capable of
analyzing non-linear programs as well, cf. \cite{hoffmann2012multivariate}, gives
for the \textsf{append} function an upper bound of $3.00 + 9.00 \cdot M$ where $M$
is the number of \textsf{::}-nodes of the 1^{st} component of the argument. Thus,
the \textsf{append} function itself has linear runtime complexity. The function
\textsf{reverse} returns an upper bound of $3.00 + 9.50 \cdot M + 4.50 \cdot M^2$.
As expected \textsf{reverse} cannot be bounded by a linear function, as its runtime
complexity growth in the order of a 2^{nd} degree polynomial[fn::The reverse
function can also be implemented using an accumulator. This implementation can then
be bound by a linear function. For such an implementation see, for instance
\url{http://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.List.html}.].

Therefore, the method introduced in \cite{Jost09carboncredits} is only capable of
analyzing the \textsf{append} function. Only using the improvements for RaML from
\cite{hoffmann2012multivariate} the input program can be fully analyzed.

#+begin_latex
\lstset{language=,numbers=none,caption={The \textsf{reverse} function implemented in RaML.}}
\begin{lstlisting}[frame=single, style=base, label={lst:raml_rev}]
let rec append xs ys =
  match xs with
    | [] -> ys
    | (x'::xs') -> x'::(append xs' ys)

let rec reverse xs =
  match xs with
    | [] -> []
    | x'::xs' -> append (reverse xs') (x'::[])
\end{lstlisting}
#+end_latex

These functions can easily be converted to a typed TRSs. However, the list has to
be defined as data type itself. Therefore, there is a recursive data type shown in
Listing \ref{lst:trs_rev} named \textsf{L} that consist of a constructor
\textsf{nil} for the empty list and a constructor \textsf{cons} that concatenates
an element to a given list. The constructor \textsf{cons} coincides with the infix
function \textsf{::} of the RaML program. Then there is a second data type defined
which specifies what kind of elements this list holds. As it is not important for
the analysis what kind of data types the list holds, cf. the upper bounds depend
only on the number of \textsf{cons} (i.e., \textsf{::}), it has no constructors.
The two functions do exactly the same as the ones specified in the RaML program:
\textsf{append} merges two lists together and \textsf{reverse} takes a list and
returns a new list with the same elements but in the opposite order. So the last
element of the input list will become the first element in the output list, etc.

#+begin_latex
\lstset{language=,numbers=none,caption={The reverse function implemented as typed TRS.}}
\begin{lstlisting}[frame=single, style=base, label={lst:trs_rev}]
(VAR x xs ys)

(DATATYPES
   Elem =     < >
   L    = µX. < nil, cons(Elem,X) >
 )

(SIGNATURES
   append  :: L x L -> L
   reverse ::     L -> L
 )

(RULES
  append(nil, ys)        -> ys
  append(cons(x,xs), ys) -> cons(x,append(xs, ys))

  reverse(nil)         -> nil
  reverse(cons(x, xs)) -> append(reverse(xs),cons(x, nil))
)
\end{lstlisting}
#+end_latex

We have a running prototype[fn:: The source code can be accessed and downloaded at
\url{https://bitbucket.org/schnecki/amortized-cost-analysis/}] implemented in
Haskell of the methods presented in \cite{hofmann2014amortised}. When the program
is run with the input typed TRSs from Listing \ref{lst:trs_rev} it prints as a
solution the content shown in Listing \ref{lst:output}. In this case the analysis
was setup to look for a solution with vector length of $2$. Therefore, the
\textsf{append} function has also vectors with length $2$, as the current prototype
does not analyze the function one after the other. However, when only
\textsf{append} is fed to the analyzer it finds a solution with vectors of length
$1$ also. Thus, \textsf{append} itself has linear runtime complexity, whereas
\textsf{reverse} has quadratic polynomial runtime complexity. The polynomials have
the same degrees as the RaML tool upper bound polynomials, however, the polynomial
bounds given by the RaML implementation are closer to the real growth. The base
constructors are used to check the uniqueness and superposition properties, cf.
Section [[Superposition and Uniqueness]], how this is done will be explained in detail
in the next Section.


#+BEGIN_LaTeX
\lstset{language=,numbers=none,caption={The output of the prototype implementing the methods as presented in \cite{hofmann2014amortised}.}}
\begin{lstlisting}[frame=single, style=base, label={lst:output}]
Solution:
---------


  append :: [L(0, 15) x L(1, 15)] -(2)-> L(0, 15)
  append :: [L(0, 15) x L(1, 15)] -(2)-> L(1, 15)

  cons :: [Elem(14, 5) x L(13, 15)] -(13)-> L(0, 15)
  cons :: [Elem(0, 1) x L(1, 0)]    -(2)->  L(1, 15)

  nil :: [] -(0)-> L(0, 15)
  nil :: [] -(0)-> L(7, 15)
  nil :: [] -(0)-> L(7, 9)
  nil :: [] -(0)-> L(14, 15)

  reverse :: [L(0, 15)] -(5)-> L(0, 15)

Base Constructors:
------------------
  cons :: [Elem(0, 1) x L(1, 0)]    -(2)->  L(1, 15)
  cons :: [Elem(14, 5) x L(13, 15)] -(13)-> L(0, 15)
  nil :: [] -(0)-> L(0, 3)
  nil :: [] -(0)-> L(7, 6)

\end{lstlisting}
#+END_LaTeX


* Implementation

We have a prototype implementing the analysis methods from
\cite{hofmann2014amortised}[fn:: Available at
\url{https://bitbucket.org/schnecki/amortized-cost-analysis}]. This section
explains the details of the current implementation.

** Parsing

First of all the system is parsed by a self-made extension of the /term-rewriting/
library[fn:: https://github.com/haskell-rewriting/term-rewriting], called
/term-rewriting-ext/[fn:: https://github.com/schnecki/term-rewriting-ext]. It
extends the /term-rewriting/ library such that data types and function signatures
can be parsed and are saved in corresponding data structures which were added for
this purpose. As stated before, the information of data types are not crucial for
the analysis itself \cite{avanzini}, except for heuristics that may be integrated
in the analysis. Therefore, at a later point the analysis implementation might be
changed to use the /term-rewriting/ library, if experiments show that the
heuristics are not needed for a scaleable analysis with this method.

** Well-Typedness Constraints

Each rule then is used once to generate the desired leafs of the proof derivation
trees. So for example the rule $\mathsf{append}(\mathsf{nil}, \mathsf{ys})
\rightarrow \mathsf{ys}$ from Listing \ref{lst:trs_rev} will create
$\tjudge{\typed{ys}{{L}^{{p(0,1)}}}}{k(0)-1+k(1)}{}{\typed{ys}{{L}^{{r(0)}}}}$,
where the functions $p$, $k$ and $r$ can be thought of as references to a global
table of signatures. In this example two signatures would be added to the global
table. The functions $p$, $k$ and $r$ in the global table represent names of the
vectors and will later be replaced by the actual vectors of the solution.

# Info: edit fields with C-c `
#+attr_latex: :placement [h!]
#+NAME: global
#+ATTR_LaTeX: :align c|lll
| Idx | Signature       |                                 |                             |
|-----+-----------------+---------------------------------+-----------------------------|
|   0 | \textsf{append} | :: L^{p(0,0)} \times L^{p(0,1)} | \xrightarrow{k(0)} L^{r(0)} |
|   1 | \textsf{nil}    | :: []                           | \xrightarrow{k(1)} L^{r(1)} |
|     | ...             |                                 |                             |

The signatures of the global table are indexed starting with 0. The first parameter
of the functions $p$, $k$ and $r$ are the index of the signature in the global
table, and for the references of shape $p(\cdot,\cdot)$ the second parameter stands
for index of the input parameter to the defined function symbol. Thus, all
vector-names in the global signature table are unique. These names will be used to
generate constraints. As stated above (see Section [[Well-Typedness]]), for each
constructor with costs $k_i$ of the lhs $\tjudge{\typed{y_1}{B_1}, \ldots,
\typed{y_l}{B_l}}{k_i}{}{\typed{l_i}{A_i}}$ must be derivable without using the
relax rule. Therefore, each constructor of the lhs will add one more proof leaf to
the list of derivable clauses.

# The other rewrite rules will add more entries in the signature:

# TODO / NEEDED?

** Type System

Using the inference rule of the type system from Figure \ref{fig:14_ts} the proofs
are generated using the rules bottom-up. First the /share/ rule is applied if
applicable. Then recursively the rules /function application/, /identity/,
/composition/, and /weake/ are applied. The rules /relax/, /supertype/ and
/subtype/ are integrated in the other rules. Thus the generated constraints will
often include $\geqslant\text{-signs}$ instead of $=\text{-signs}$. The function
application rule adds an entry to the global signature table, whereas the other
inference rules do not. So for instance the rewrite rule
$\mathsf{append}(\mathsf{nil}, \mathsf{ys}) \rightarrow \mathsf{ys}$ uses the
identity inference rule and does not alter the global signature table:

#+BEGIN_LaTeX
  \begin{center}
   $\infer{\tjudge{\typed{ys}{{\mathsf{L}^{p(0,1)}}}}{{k(0)}-1+k(1)}{}{\typed{ys}{{\mathsf{L}^{r(0)}}}}}{}$
  \end{center}\\
#+END_LaTeX

However, some constraints follow by applying the inference rules. In this case,
according to the identity rule from the type system, the types from the lhs and the
rhs of the judge have to be equal. When the /super-/ and /subtype/ rules are
integrated, this yields constraint (1). The costs $k(0)-1+k(1)$ must be equal to
$0$. However, when the relax rule is integrated constraint (2) is gained. This is
done for all clauses that need to be derived, coming up with a set of constraints
and an extended global signature table.

#+BEGIN_LaTeX
\begin{align}
p(0,1) & \geqslant r(0)\\
k(0)-1+k(2) & \geqslant 0
\end{align}
#+END_LaTeX

** Other Constraints

Several other constraints need to be added to ensure that the derivations are
correct, superposition and uniqueness are satisfied, and the potential of the
constructors does not grow to steep.

First for every signature of a defined function symbol in the global signature
table that was generated from the well-typedness proof leaf generation, explained
in Section [[Well-Typedness Constraints]], constraints are added such that all vectors
are equal. In other words, the signatures of the same defined function symbol as
root which are directly gained from the lhs of the rewrite rules must be equal.
These signatures will in the following be called base signatures of the defined
function symbols. Where \citeauthor{hoffmann2011types} generated a set of possible
signatures \cite[p. 93]{hoffmann2011types}, we use simple
$\geqslant\text{-constraints}$ to ensure that all signatures are compatible with
the corresponding base signatures (and thus their derivations), cf. Section
[[Well-Typedness]]. Hence, any occurrence in the rhs of a rewrite rule must have (a)
parameter types with at least the same annotation as the base signatures do, (b)
costs with at least the same as the base signatures do and (c) the return type must
not be greater than the one of the base signatures. These constraints follow by
applying the /super-/ and /subtype/ inference rules to the signatures gained by
applying the type system.

To ensure superposition and uniqueness (cf., Section [[Superposition and Uniqueness]])
so called base constructor signatures are added. For each constructor and every
dimension of the vectors a base signature is added. So for instance, in the
\textsf{reverse} example for the constructor \textsf{cons} for a maximum vector
length of $2$, the following base constructors were added to the constraint system:

#+BEGIN_LaTeX
\lstset{language=,numbers=none,caption={Base signatures for \textsf{cons}-constructor of the  \textsf{reverse} example.}}
\begin{lstlisting}[frame=single, style=base, label={lst:output}]
  cons_0 :: [Elem(0, 1) x L(1, 0)]    -(2)->  L(1, 15)
  cons_1 :: [Elem(14, 5) x L(13, 15)] -(13)-> L(0, 15)
\end{lstlisting}
#+END_LaTeX

Each \textsf{cons}-signature in the global signature table must now be a linear
combination of these two base signatures: $c_1 \cdot \text{cons\_0} + c_2 \cdot
\text{cons\_1}$, where $c_1, c_2$ are arbitrary constants and \text{cons\_0} and
\text{cons\_1} are the base signatures. This ensures that the superposition
principle is satisfied. Further constraints are added, such that, whenever the rhs
annotations of two constructor signatures of the same identifier are equal, then
all other annotation must be equal as well. This ensures that the uniqueness
property holds.

Finally, constraints are added to bound the growth of the potential of the
constructor symbols, as it is defined in Theorem 3.2 in
\cite{hofmann2014amortised}.

** Solving

Just before the constraint problem is written to a file, the variables are lifted
to vectors of the desired length. Currently the system is not able to use different
vector lengths for different function signatures. So for example, if the maximum
vector length is set to $2$ the system will convert all constraints to vectors of
size $2$, e.g., $p(0,1) & \geqslant r(0)$ become the constraints $p_0(0,1)
\geqslant r_0(0) \land p_1(0,1) \geqslant r_1(0)$. These constraints are then
written to a file and a SMT solver is called. In our case this is the SMT solver
called /z3/[fn:: See \url{https://github.com/Z3Prover/z3/wiki}.] which is developed
by Microsoft.

The result is parsed and the variables in the global signature table are replaced
with the actual solution vectors. Finally, the solution is printed on the screen.
It is possible to display the inference trees computed by the system[fn:: See
\textsf{-h} for the usage info.].


* Preliminary Experiments

This section shows first experimental results of the implementation of
\cite{hofmann2014amortised}. During the experiment executions a bug was detected:
The TRS given in Listing \ref{lst:exp}, which calculates the value of $2^x$ where
$x$ is the input parameter, was typed with a vector length of $2$, thus inferring
quadratic runtime. However, the runtime complexity of this calculation cannot be
bound by a polynomial. Therefore, the prototype in the current state is unsound.
Thus, the execution times of the experiments have to be considered with caution and
further should not be compared to similar analysis tools yet. Nonetheless, we
expect to have similar execution times after the problem was located and fixed.

#+BEGIN_LaTeX
\lstset{language=,numbers=none,caption={TRS calculating the $2^x$, where $x$ is the input paramter.}}
\begin{lstlisting}[frame=single, style=base, label={lst:exp}]
  (VAR x)

  (DATATYPES
    Nat = µX.< 0, s(X) >
  )

  (SIGNATURES
    d   :: Nat -> Nat
    exp :: Nat -> Nat
  )

  (RULES
    d(0)    -> 0
    d(s(x)) -> s(s(d(x)))
    exp(0)    -> s(0)
    exp(s(x)) -> d(exp(x))
  )
\end{lstlisting}
#+END_LaTeX

The program was run on a small set of test TRSs on a Laptop with a
Intel\textsuperscript{\textregistered} Mobile Core\textsuperscript{\texttrademark}
i7--3840QM Processor with 32 GB DDR3 RAM running ArchLinux (64-bit).


Table \ref{tbl:linear} shows problems which could be shown to have linear runtime
complexity. The execution times are an average of 10 independent executions. Table
\ref{tbl:quadratic} displays problems that have, according to the (unsound) tool,
quadratic runtime complexity. In sum there are 15 solvable linear problems, 22
solvable quadratic problems, 14 problems were infeasible (the SAT solver returned
'unsat') and 22 problems ran into the timeout of 30 seconds. Further, the problems
which were infeasible or ran into a timeout, could also not be solved when the
program was asked for a cubic or even higher degree polynomial as a runtime
complexity bound.


Adding to the obvious need of making the tool sound, the results also demonstrate
the need for improvements in the sense of scaleability as 22 problems raised a
timeout. This shows clearly, that the tool needs to be optimized, to also work for
more complex examples. All test TRSs are also available on the git-repository[fn::
See \url{https://bitbucket.org/schnecki/amortized-cost-analysis}.].

# info: edit fields with C-c `
#+attr_latex: :placement [h!]
#+NAME: tbl:linear
#+CAPTION: Execution times of the analysis of the TRSs with linear runtime complexity. The execution time is displayed in milliseconds and is an average over 10 independent executions.
#+ATTR_LaTeX: :align | l | r | r | r | r | r | r | r | r | r | r | r | c |
|----------------------------------+--------+---------|
| Example                          | t [ms] | RetVal  |
|----------------------------------+--------+---------|
| addition.trs                     |   52.4 | Success |
| append.trs                       |   53.3 | Success |
| id.trs                           |   40.3 | Success |
| list.trs                         |  177.5 | Success |
| minus.trs                        |   38.8 | Success |
| queue.trs                        |  761.7 | Success |
| quotient.trs                     |  136.4 | Success |
| reverse.trs                      |   49.0 | Success |
| typed-div.trs                    |  151.9 | Success |
| typed-flatten.trs                |   89.9 | Success |
| typed-jones1.trs                 |   60.1 | Success |
| typed-jones2.trs                 |   74.9 | Success |
| typed-jones4.trs                 |   97.3 | Success |
| typed-jones6.trs                 |  113.7 | Success |
| typed-rationalPotential.raml.trs |  386.4 | Success |
|----------------------------------+--------+---------|

# Info: edit fields with C-c `
#+attr_latex: :placement [h!]
#+NAME: tbl:quadratic
#+CAPTION: Execution times of the analysis of the TRSs with quadratic runtime complexity. The execution time is displayed in milliseconds and is an average over 10 independent executions.
#+ATTR_LaTeX: :align | l | r | r | r | r | r | r | r | r | r | r | r | c |
|--------------------------+--------+---------|
| Example                  | t [ms] | RetVal  |
|--------------------------+--------+---------|
| exp.trs                  |  320.7 | Success |
| infeasible.trs           |  160.7 | Success |
| mult3.trs                |  197.3 | Success |
| reverse.trs              |  321.3 | Success |
| typed-appendAll.raml.trs |  723.7 | Success |
| typed-bits.trs           | 1151.7 | Success |
| typed-dcquad.trs         |  772.3 | Success |
| typed-exp.trs            |  360.9 | Success |
| typed-isort.trs          |  980.7 | Success |
| typed-jones5.trs         |  277.6 | Success |
| typed-lcs-safe.trs       | 1900.9 | Success |
| typed-lcs.trs            | 1611.5 | Success |
| typed-minsort.raml.trs   | 5114.5 | Success |
| typed-pairs.trs          |  640.3 | Success |
| typed-qbf.trs            | 2199.2 | Success |
| typed-quad.trs           |  278.7 | Success |
| typed-quicksort-nat.trs  | 3522.5 | Success |
| typed-reverse.trs        |  315.8 | Success |
| typed-sat.trs            | 2695.7 | Success |
| typed-shuffleshuffle.trs |  959.5 | Success |
| typed-shuffle.trs        |  772.0 | Success |
| typed-subtrees.raml.trs  |  495.4 | Success |
|--------------------------+--------+---------|

* Future Work

First the detected bug needs to be fixed. Then for the next couple of weeks we will
be running experiments to get an idea how stable the prototype is. Afterwards,
still in the near future we will be implementing a graph analysis to identify
strongly connected components of the input system. This will allow separation of
analyses of the different functions of the input program. The gained constraint
problems should be simpler and therefore faster solved by the SMT solver. This
ought to increase the scaleability of the method. Further, we will try out
heuristics, like the additive shift as presented in \cite{hofmann2014amortised}.
According to the results, either the heuristics will be integrated into the system,
or the need for data types will be removed. Adding to this, once stable we plan to
integrate the tool into TcT \cite{avanzini2016tct}. Further, it is planned to lift
this univariate implementation to the multivariate analysis as presented in
\cite{hofmann_et_al:LIPIcs:2015:5167}. However, to this point unknown problems will
most likely emerge by this lifting, and therefore this is planned for later.

* Conclusion

This seminar report presented the evolution of amortized resource analysis
techniques over the past decades with a special focus on the developments for TRS
analyses. We showed how the ideas used in RaML were converted to be compatible with
TRS and how these methods than were improved to be usable with generic data types
and to gain an analysis of univariate polynomial bounds for (typed) TRSs. This
includes the most important ideas, the type system, the potential function, and
superposition and uniqueness concepts. Further, we have explained the current
status of the prototype. The system is able to type programs which also admit
non-linear runtime complexity. However the tool is not yet stable enough as it was
not extensively tested until today. The implementation was explained and
illustrated. Using well-typedness the desired leafs of the inference tree proofs
are generated. This includes constructor proof derivations. Then the type system is
used to create the proofs using the inference rules bottom-up. By doing so
constraints are simultaneously generated and collected. A global signature table is
used to be able to refer and uniquely identify each signature that is used during
the proof creation. Several other constraints are added to ensure the correctness
of the derivations. And finally a SMT solver is called to solve the constraint
problem.

In the future the system will be tested, heuristics tried out, and improvements
will be made, before the tool can be integrated into TcT, a runtime complexity
analyzer built by the Computational Logic group at the University of Innsbruck[fn::
See \url{http://cl-informatik.uibk.ac.at}]. We hope to be able to add value to TcT
during the upcoming months and believe that runtime complexity analysis will be an
important field of study for the future, for instance when supporting programmers
at constructing efficient algorithms using functional programming languages.


\vfill


* References                                                  :ignoreheading:
# ------------------------------ References ------------------------------
\pagebreak[4]

\bibliographystyle{plain}
\bibliography{biblio}
